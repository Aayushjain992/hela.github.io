<!DOCTYPE html>
<html>
<head>
  <title>Copy Variable Text to Clipboard</title>
</head>
<body>
  <button onclick="copyVariableText1()">read</button>
  <button onclick="copyVariableText2()">comb</button>
  <button onclick="copyVariableText3()">mapre</button>
  <button onclick="copyVariableText4()">conf</button>
  <button onclick="copyVariableText5()">temview</button>
  <button onclick="copyVariableText6()">udf</button>
  <button onclick="copyVariableText7()">lin</button>
  <button onclick="copyVariableText8()">kmean</button>
  <button onclick="copyVariableText9()">log</button>
  <button onclick="copyVariableText10()">naiv</button>
  <button onclick="copyVariableText11()">stream</button>
  <button onclick="copyVariableText12()">count manip</button>


  <script>
    function copyVariableText1() {
      // Your variable containing the text data
!pip install pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder.appName('test').getOrCreate()
df = spark.read.option('header', True).csv('/content/Employee.csv')
df.show()
df.select('salary').show(5)
df.printSchema()
panda_df = df.toPandas()
panda_df.head()

hdfs namenode -format
start-dfs.cmd
start-yarn.cmd
jps
hdfs dfs -ls /
hdfs dfsadmin -report 
hdfs dfs -mkdir /user/hadoop
hdfs dfs -put C:\ha\p.txt /user/hadoop

    }

    function copyVariableText2() {
      // Your variable containing the text data
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
spark = SparkSession.builder.appName('CombineDataframe').getOrCreate()
data1 = [(1, 'xy', 2), (2, 'ys', 3), (3, 'ko', 5)]
data2 = [(4,'s', 5), (5, 'rp', 2), (6, 'os', 2)]
columns = ['ID', 'Name', 'Age']
df1 = spark.createDataFrame(data1, columns)
df2 = spark.createDataFrame(data2, columns)
df1.show()
df2.show()
combine_df = df1.union(df2)
combine_df.show()
data4 =[(1, 'js'), (2, 'la'), (3, 'us'), (4, 'ca'), (5, 'ma'), (3, 'an')]
columns4 = ['ID', 'City']
df4 = spark.createDataFrame(data4, columns4)
df4.show()
join_df = combine_df.join(df4, on='ID', how='inner')
join_df.show()
df1 = df1.withColumn('country', col('Age'))
df1.show()

//file
df1= spark.read.option('header',True).csv('/content/Employee.csv')
df2= spark.read.option('header',True).csv('/content/Department.csv')
spar1 = SparkSession.builder.appName('JoindiffCSV').getOrCreate()
df6 = spar1.read.option('header',True).csv('/content/Department.csv')
df6.show()
comb_df2 = df.join(df6,on = 'Department_Id',how = 'inner')
comb_df2.show()

 }

    function copyVariableText3() {
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("SalesTotal").getOrCreate()
sc = spark.sparkContext 
sales_data = [
    ("Laptop", 1000),
    ("Mobile", 500),
    ("Laptop", 1200),
    ("Tablet", 700),
    ("Mobile", 450),
    ("Tablet", 650),
    ("Laptop", 1100),
    ("Mobile", 520),
]
rdd = sc.parallelize(sales_data)
mapped_rdd = rdd.map(lambda x: (x[0], x[1]))
total_sales_rdd = mapped_rdd.reduceByKey(lambda a, b: a + b)
result = total_sales_rdd.collect()
for product, total in result:
    print(f"{product}: ${total}")
spark.stop()
    }
    function copyVariableText4() {
from pyspark.sql import SparkSession
spark=SparkSession.builder.appName("SparkSessionExample").config("SparkConfig","config-value").getOrCreate()
from pyspark.sql import Row
data=[Row(name="al",Age=34),Row(name="r",Age=30),Row(name="sd",Age=35)]
df=spark.createDataFrame(data)
df.show()
df.printSchema()
#user
num_rows = int(input("Enter the number of rows: "))
data=[]
for i in range(num_rows):
    name = input(f"Enter the name  : ")
    age = int(input(f"Enter the age : "))
    city=input(f"Enter the city : ")
    data.append((name,age,city))

column=["name","age","city"]
df=spark.createDataFrame(data,schema=column)
df.show()
df.printSchema()
df.filter(df.age>24).show()
df.where(df.city=="ms").show()
output_path="1.csv"
df.write.option("header",True).csv(output_path)
    }
    function copyVariableText5() {
from pyspark.sql import SparkSession
from pyspark.sql import Row
spark=SparkSession.builder.appName('TemperoryViewExample').getOrCreate()
data=[Row(id=1,name="ABC",age=25,city="ss"),Row(id=2,name="DEF",age=35,city="hi"),Row(id=3,name="GHI",age=15,city="ra"),Row(id=4,name="XYZ",age=38,city="js")]
df=spark.createDataFrame(data)
df.show()
df.createOrReplaceTempView("Employee")
result=spark.sql("select * from Employee")
result.show()
result1=spark.sql("select * from Employee where age>30")
result1.show()
#Sales 

data=[Row(1,"Electronic",1000,"2025-03-01"),
      Row(2,"Electronic",3000,"2025-01-01")]
columns=["sales_id","category","amount","date"]
df=spark.createDataFrame(data,columns)
df.createOrReplaceTempView("Sales")
result=spark.sql("select * from Sales")
result2=spark.sql("select category,AVG(amount) as Avg_Amt from Sales group by category")
result2.show()
result3=spark.sql("select * from Sales order by amount desc limit 1")
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
df_query1=result2.toPandas()
#barplot
plt.figure(figsize=(8,5))
sns.barplot(x="category",y="Avg_Amt",data=df_query1)
plt.title("Sales Amount by Category ")
plt.xlabel("Category")
plt.ylabel("Sales Amount")
plt.show()
    }
    function copyVariableText6() { 
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

spark= SparkSession.builder.appName("UDF_Example").getOrCreate()
data=[("os",65),("pa",12),("jo",45)]
columns=["name","age"]
df=spark.createDataFrame(data,schema=columns)
df.printSchema()
def age_group(age):
  if age<18:
    return "Minor"
  elif age >= 18 and age <= 60:
    return "Adult"
  else:
    return "Senior Citizen"

age_group_udf=udf(age_group,StringType()) 
df=df.withColumn("age_group",age_group_udf(df["age"]))
df.show()
    }
    function copyVariableText7() {
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

spark = SparkSession.builder.appName("RegressionExample").getOrCreate()
data=[(1,100),(2,110),(3,120),(4,130),(5,105)]
columns=["feature","target"]
df=spark.createDataFrame(data,schema=columns)
df.show()
assembled=VectorAssembler(inputCols=["feature"],outputCol="features")
df=assembled.transform(df).select("features","target")
df.show()
lr=LinearRegression(featuresCol="features",labelCol="target")
lr_model=lr.fit(df)
print("Coefficients:",lr_model.coefficients)
print("Intercept:",lr_model.intercept)
training_summary=lr_model.summary
print("Root Mean Squared error:",training_summary.rootMeanSquaredError)
print("R2:",training_summary.r2)
prediction=lr_model.transform(df)
prediction.select("features","target","prediction").show()
    }
    function copyVariableText8() {
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
spark=SparkSession.builder.appName('Kmeans').getOrCreate()
data=[(1.0,2.0),(1.5,1.8),(5.0,8.0),(8.0,8.0),(1.0,0.6),(9.0,11.0)]
columns=["sub1","sub2"]
df=spark.createDataFrame(data,columns)
df.show()
assembler=VectorAssembler(inputCols=["sub1","sub2"],outputCol="features")
res=assembler.transform(df)
res.show()
kmeans=KMeans(k=2,seed=1)
model=kmeans.fit(res)
prediction=model.transform(res)
prediction.show()
for center in model.clusterCenters():
  print(center)
    }
    function copyVariableText9() {
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import CountVectorizer,IDF,Tokenizer
spark=SparkSession.builder.appName('Log').getOrCreate()
data=[(1,"Win a free Iphone"),(1,"You won a lottery"),(0,"Hello Lets meet for lunch"),(0,"Dont forget to complete the assignment")]
columns=["label","text"]
df=spark.createDataFrame(data,columns)
df.show()
tokenizer=Tokenizer(inputCol="text",outputCol="Tokens") # Now Tokenizer should be defined
res=tokenizer.transform(df)
res.show()
vectorizer=CountVectorizer(inputCol="Tokens",outputCol="features")
Vector_model=vectorizer.fit(res)
vec=Vector_model.transform(res)
vec.show()
idf=IDF(inputCol="features",outputCol="idf_features")
idf_model=idf.fit(vec)
res=idf_model.transform(vec)
res.show()
df1=res.select("label","idf_features")
lr=LogisticRegression(featuresCol="idf_features",labelCol="label")
model=lr.fit(df1)
prediction=model.transform(df1)
prediction.show()
    }
    function copyVariableText10() {
from pyspark.sql import SparkSession
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

spark = SparkSession.builder.appName("NaiveBayesWeather").getOrCreate()
data = [
    ("Sunny", 30, 50, 10),
    ("Rainy", 22, 85, 20),
    ("Cloudy", 25, 70, 15),
    ("Sunny", 35, 40, 8),
    ("Rainy", 20, 90, 25),
    ("Cloudy", 28, 65, 12),
    ("Sunny", 32, 45, 7),
    ("Rainy", 19, 95, 30),
    ("Cloudy", 26, 75, 18),
]
columns = ["weather", "temperature", "humidity", "wind_speed"]
df = spark.createDataFrame(data, columns)
df.show()
indexer = StringIndexer(inputCol="weather", outputCol="label")
df = indexer.fit(df).transform(df)
df.show()
vector_assembler = VectorAssembler(inputCols=["temperature", "humidity", "wind_speed"], outputCol="features")
df = vector_assembler.transform(df).select("label", "features")

train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)
nb = NaiveBayes(modelType="multinomial")
model = nb.fit(train_data)
predictions = model.transform(test_data)
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Test Accuracy: {accuracy:.2f}")
predictions.select("label", "features", "prediction").show()
    }
    function copyVariableText11() {
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("WordCount").getOrCreate()
sc = spark.sparkContext  
data = [
    "hello world",
    "hello pyspark",
    "pyspark map reduce example",
    "reduce and map are powerful",
]
rdd = sc.parallelize(data)
mapped_rdd = rdd.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1))
word_counts = mapped_rdd.reduceByKey(lambda a, b: a + b)
result = word_counts.collect()
for word, count in result:
    print(f"{word}: {count}")
spark.stop()
    }
    function copyVariableText12() {
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode,split
from pyspark.sql.functions import col
spark=SparkSession.builder.appName('Word').getOrCreate()
data=[("Hello World",),("Hello , How are you",),("I am fine =",)]
columns=["text"]
df=spark.createDataFrame(data,columns)
df.show(truncate=False)
word_df=df.withColumn("words",explode(split(col("text")," ")))
word_df.show()
word_count=word_df.groupBy("words").count().orderBy(col('count').desc())
word_count.show()
    }

  </script>
</body>
</html>
